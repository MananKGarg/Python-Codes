Measuring model quality helps in iteratively improving our model.

## What is Model Validation

* In most,(though not all) applications, relative measure of model quality is predictive accuracy which means how close are mdel's predictions to what really happens.
* One big mistake is to make predictions with the training data and compare those predictions to the target values in the training data.We'll see the underlying problem with this approach.

But how do we check quality?

* Firstly, we need to express model quality in an understandable (mathematical) way. For example, if we have a model of 10000 houses, there might be some good and some bad predictions, so we need to summarise everything in a single metric
* We'll start with a metric called as **Mean Absolute Error (MAE)**

```python
error = actual - predicted

MAE = average of |errors|
```

Firstly, we need a model, so the model is
```python
# Data Loading Code Hidden Here
import pandas as pd

# Load data
melbourne_file_path = '../input/melbourne-housing-snapshot/melb_data.csv'
melbourne_data = pd.read_csv(melbourne_file_path) 
# Filter rows with missing price values
filtered_melbourne_data = melbourne_data.dropna(axis=0)
# Choose target and features
y = filtered_melbourne_data.Price
melbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea', 
                        'YearBuilt', 'Lattitude', 'Longtitude']
X = filtered_melbourne_data[melbourne_features]

from sklearn.tree import DecisionTreeRegressor
# Define model
melbourne_model = DecisionTreeRegressor()
# Fit model
melbourne_model.fit(X, y)
```

Now, we have our model, so this is how we calculate MAE.

```python

from sklearn.metrics import mean_absolute_error

predicted_home_prices = melbourne_model.predict(X)
mean_absolute_error(y, predicted_home_prices)

output - 
434.71594577146544
```

## The problem with 'In-Sample' scores

The measure we just did was 'in sample' score i.e. we used a single sample of houses for both building as well as evaluating the model. But why's it bad?

* imagine in a real estate business, door color is unrelated to price 
* but in our training data, houses with green doors were expensive. Model just sees patterns and it would conclude that houses with green doors are expensive.
* There would be no problem with prediction o training data as it will comply with the prediction
* but as soon as the model hits the real world, it would get very inaccurate
* A simple solution to this is to leave some of the data from the training data while building the model and afterwards use that to test model's accuracy. This data is called **Validation Data**

## Coding It

Scikit has a function train_test_split to break up the data into two pieces. One of which we use to train the model and other as validation data to calculate MAE

```python
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split

# split into training and validation data, for both target and features
# split is based on a random number generator. Supplying a numeric value to the random state argument is necessary as it ensures that we get the same split every time we run the script

train_X, val_X, train_y, val_y = train_test_split (X, y, random state = 0)
melbourne_model = DecisionTreeRegressor()  # Defining model

melbourne_model.fit(train_X, train-y) # Fit Model

val_predictions = melbourne_model.predict(val_X) # get predicted price on validtion data
print(mean_absolute_error( val_y, val_predictions)

Output - 
259556.7211103938
```

Wooahh, we see the error bounced off from 500 to 250000 dollars. This is the difference between a model that is almost exactly right and one that is unusable.We can improve the model in many ways, one way is to choose better features( not choosing door colour as one!!)



























