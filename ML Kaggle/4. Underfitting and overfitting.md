## Experimenting with different models

Now that we can measure model accuracy, we can experiment with alternative models and see which ones give the best results. We can see in Scikit learn's [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) has many options one of which is to decide tree's depth i.e. how many splits it makes before coming to a prediction.

* As the tree gets deeper, dataset gets sliced up into leaves with fewer houses. for example, at 10th split there would be 1024 groups of houses.

* **Overfitting** - If we divide houses in many leaves, we have fewer houses in each leaf. So now our prediction would be quite close to home's actual values but it would perform poor for new data as each prediction is based only on a few houses.

* **Underfitting** - On the other hand let us say we divide only in 2-4 leafs, predictions may be far off, even for the training data as each group has a wide variety of houses. So when a model fails to capture important distinction in model , it performs poorly even in the training set, that is called underfitting.

* So our goal is to find that sweet spot between overfitting and underfitting where the error is minimum. Visually, we want the low point of the red validation curve.

![2q85n9s](https://user-images.githubusercontent.com/62146744/78992086-18b3da80-7b58-11ea-9f6c-e840979ecefb.png)



























































