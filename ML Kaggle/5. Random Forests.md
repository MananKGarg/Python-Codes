# Random Forests

Decison tree models have the problem of underfitting and overfitting. Many models face this problem and thus there are a number of models which use clever startegies to tackle this. One of it is the **Random Forests**

Random forest uses many trees and makes prediction by averaging the results of componenet trees. It has much better predictive acuracy than single decision tree and works well with default parameters.

There are models with even better performance but many of those are sensitive to getting the right parameters.

## Example

From the previous topic, we've already seen the code to load the data a few times. At the end of data loading, we have following variables.

>* train_X
>* val_X
>* train_y
>* val_y

We'll use them again and this time build the model using ```python RandomForestregressor ``` class instead of ```python DecisionTreeRegressor```

```python

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error

forest_model = RandomForestRegressor( random_state = 1)
forest_model.fit(train_X, train_y)
preds = forest_model.predict(val_X)
print(mean_absolute_error(preds,val_y)

output - 

202888.18157951365

```
## Conclusion

This is improvement from 250,000 of Decision Tree. However there is room for improvement and we can change parameters similar to depth in Decision Tree but the best thing about Random forests is they work good even without that tuning.

We'll further learn XGBoost model which provides better performance when tuned in with right parameters.

























































